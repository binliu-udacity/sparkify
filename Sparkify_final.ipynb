{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used on AWS EMR\n",
    "# import packages\n",
    "# import os\n",
    "# import re\n",
    "# import sys\n",
    "# print(\"start SPARK application\")\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark/\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/mnt/anaconda3/bin/python3.7\"\n",
    "# spark_home = os.environ.get('SPARK_HOME', None)\n",
    "# sys.path.insert(0, spark_home + \"/python\")\n",
    "# sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-src.zip'))\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"yarn\") \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#     .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "#     .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "#     .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "#     .config(\"spark.driver.memory\", \"24g\") \\\n",
    "#     .config(\"spark.driver.cores\", \"4\") \\\n",
    "#     .config(\"spark.executor.cores\", \"4\") \\\n",
    "#     .config(\"spark.executor.memory\", \"24g\") \\\n",
    "#     .config(\"spark.kryoserializer.buffer.max\", \"2000m\") \\\n",
    "#     .config(\"spark.network.timeout\", \"360000\") \\\n",
    "#     .config(\"spark.dynamicAllocation.minExecutors\", \"25\")\\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 72000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType, DoubleType, Row, DateType\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session for LOCAL \n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Sparkify\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"medium-sparkify-event-data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('s3://vf-bdc-uk-prod-eu-west-1-datascientistscratch/ElifSurmeli/' + event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df.take(5), columns=df.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(df):\n",
    "    \"\"\"\n",
    "    Removes special characters from StringType columns \n",
    "    (special chars. except , and - which might be useful for splitting columns)\n",
    "    Casts miliseconds ts column and creates two new columns with timestamptype and datetype,\n",
    "    which will be used for further processing\n",
    "    Extracts month from ts\n",
    "    Casts user id to LongType\n",
    "    Splits location field and takes only state name as location\n",
    "    :param df: Spark DataFrame\n",
    "    :return df preprocessed Spark DataFrame\n",
    "    \"\"\"\n",
    "    #Â cleanse stringtype fields from special characters\n",
    "    for field in df.schema.fields:\n",
    "        if field.dataType==StringType():\n",
    "            df = df.withColumn(field.name, regexp_replace(field.name, '[^a-zA-Z0-9\\,\\-]', ''))\n",
    "    \n",
    "    df = df.withColumn('interaction_time', from_unixtime(col('ts').cast(LongType())/1000).cast(TimestampType()))\n",
    "    df = df.withColumn('month', month(col('interaction_time')))\n",
    "    df = df.withColumn('date', from_unixtime(col('ts')/1000).cast(DateType()))\n",
    "    df = df.withColumn('userId', col('userId').cast(LongType()))\n",
    "    df = df.filter(col('userId').isNotNull())\n",
    "    df = df.withColumn('location', split(col('location'),',').getItem(1))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(df):\n",
    "    \"\"\"\n",
    "    Calculates features from existing fields\n",
    "    Adds label column\n",
    "    :param df: spark DataFrame\n",
    "    \"\"\"\n",
    "   \n",
    "    label_df = df.withColumn('label',\n",
    "                             when((col('page').\\\n",
    "                                   isin(['Cancellation Confirmation','Cancel'])) | \\\n",
    "                                  (col('auth')=='Cancelled'),1 ).\\\n",
    "                             otherwise(0)).\\\n",
    "    groupby('userId').agg(sum('label').alias('label')).\\\n",
    "    withColumn('label', when(col('label')>=1 ,1).otherwise(0))\n",
    "\n",
    "    df = df.join(label_df, on='userId')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registered_days(df):\n",
    "    \"\"\"\n",
    "    Calculates number of days between registration to last interaction\n",
    "    :param df: spark DataFrame\n",
    "    :return df with calculated column\n",
    "    \"\"\"\n",
    "    last_interaction_df =  df.groupBy('userId').agg(max('ts').alias('last_interaction'))\n",
    "\n",
    "    df = last_interaction_df.join(df, on='userId').withColumn('registered_days', ((col('last_interaction')-col('registration'))/86400000).cast(IntegerType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_level(df):\n",
    "    \"\"\"\n",
    "    Finds the latest level of each user\n",
    "    :param df: spark DataFrame\n",
    "    :return df with latest level assigned to each user\n",
    "    \"\"\"\n",
    "    level_df = df.orderBy('ts', ascending=False).groupBy('userId').agg(first('level').alias('valid_level'))\n",
    "    \n",
    "    df = df.drop('level')\n",
    "    df = df.join(level_df, on='userId')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_length(df):\n",
    "    \"\"\"\n",
    "    Calculates average item length for each user\n",
    "    :param df: spark DataFrame\n",
    "    :return df with average length column\n",
    "    \"\"\"\n",
    "    avg_length_df = df.groupBy('userId').avg('length').withColumnRenamed('avg(length)', 'length')\n",
    "    \n",
    "    df = df.drop('length')\n",
    "    df = df.join(avg_length_df, on='userId')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionid_aggregates(df):\n",
    "    \"\"\"\n",
    "    Calculates daily and monthly average of distinct sessionId for each user\n",
    "    :param df:spark DataFrame\n",
    "    :return daily and monthly aggregates DataFrame\n",
    "    \"\"\"\n",
    "    daily_session_df = df.groupby('userId','date').agg(countDistinct('sessionId')).\\\n",
    "    groupBy('userId').avg('count(DISTINCT sessionId)').\\\n",
    "    withColumnRenamed('avg(count(DISTINCT sessionId))', 'avg_daily_sessions')\n",
    "    \n",
    "    monthly_session_df = df.groupby('userId','month').agg(countDistinct('sessionId')).\\\n",
    "    groupBy('userId').avg('count(DISTINCT sessionId)').\\\n",
    "    withColumnRenamed('avg(count(DISTINCT sessionId))', 'avg_monthly_sessions')\n",
    "    \n",
    "    return daily_session_df.join(monthly_session_df, on='userId')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_duration_aggregates(df):\n",
    "    \"\"\"\n",
    "    Calculates daily and monthly average of distinct sessionId for each user\n",
    "    :param df:spark DataFrame\n",
    "    :return daily and monthly aggregates DataFrame\n",
    "    \"\"\"\n",
    "    daily_session_duration_df = df.groupby('userId','date','sessionId').\\\n",
    "    agg(max('ts').alias('session_end'), min('ts').alias('session_start')).\\\n",
    "    withColumn('session_duration_sec', (col('session_end')-col('session_start'))*0.001).\\\n",
    "    groupby('userId','date').\\\n",
    "    avg('session_duration_sec').\\\n",
    "    groupby('userId').\\\n",
    "    agg(mean('avg(session_duration_sec)').alias('avg_daily_session_duration')).\\\n",
    "    orderBy('userId', ascending=False)\n",
    "    \n",
    "    monthly_session_duration_df = df.groupby('userId','month','sessionId').\\\n",
    "    agg(max('ts').alias('session_end'), min('ts').alias('session_start')).\\\n",
    "    withColumn('session_duration_sec', (col('session_end')-col('session_start'))*0.001).\\\n",
    "    groupby('userId','month').\\\n",
    "    avg('session_duration_sec').\\\n",
    "    groupby('userId').\\\n",
    "    agg(mean('avg(session_duration_sec)').alias('avg_monthly_session_duration')).\\\n",
    "    orderBy('userId', ascending=False)\n",
    "    \n",
    "    return daily_session_duration_df.join(monthly_session_duration_df, on='userId')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_aggregates(df):\n",
    "    \"\"\"\n",
    "    Calculates monthly, daily averages for each user,for each item \n",
    "    :param df: spark DataFrame\n",
    "    :return daily and monthly aggregates DataFrame\n",
    "    \"\"\"\n",
    "    daily_item_df = df.groupby('userId','date').agg(max('itemInSession')).\\\n",
    "    groupBy('userId').avg('max(itemInSession)').\\\n",
    "    withColumnRenamed('avg(max(itemInSession))', 'avg_daily_items')\n",
    "    \n",
    "    monthly_item_df = df.groupby('userId','month').agg(max('itemInSession')).\\\n",
    "    groupBy('userId').avg('max(itemInSession)').\\\n",
    "    withColumnRenamed('avg(max(itemInSession))', 'avg_monthly_items')\n",
    "    \n",
    "    return daily_item_df.join(monthly_item_df, on='userId')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_event_aggregates(df):\n",
    "    \"\"\"\n",
    "    Calculates monthly, daily averages for each user,for each page event except the ones that include \"cancel\" \n",
    "    :param df: spark DataFrame\n",
    "    :return daily and monthly aggregates DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    listOfDistinctPages = [row.page for row in df.select('page').distinct().collect()]\n",
    "    listOfDistinctPages.remove('Cancel')\n",
    "    listOfDistinctPages.remove('CancellationConfirmation')\n",
    "    \n",
    "    daily_page_event_df = df.groupby('userId','date').pivot('page').count()\n",
    "    exp_dict={}\n",
    "    for page in listOfDistinctPages:\n",
    "        exp_dict.update({page:'mean'})\n",
    "\n",
    "    daily_page_event_df = daily_page_event_df.join(daily_page_event_df.groupBy('userId').agg(exp_dict).fillna(0), on='userId')\n",
    "\n",
    "    for page in listOfDistinctPages:\n",
    "        daily_page_event_df = daily_page_event_df.drop(page)  \n",
    "        daily_page_event_df = daily_page_event_df.withColumnRenamed('avg({})'.format(page), 'avg_daily_{}'.format(page))\n",
    "\n",
    "    daily_page_event_df = daily_page_event_df.drop('Cancel','CancellationConfirmation','date').drop_duplicates()\n",
    "    \n",
    "    \n",
    "    monthly_page_event_df = df.groupby('userId','month').pivot('page').count()\n",
    "\n",
    "    monthly_page_event_df = monthly_page_event_df.join(monthly_page_event_df.groupBy('userId').agg(exp_dict).fillna(0), on='userId')\n",
    "\n",
    "    for page in listOfDistinctPages:\n",
    "        monthly_page_event_df = monthly_page_event_df.drop(page)    \n",
    "        monthly_page_event_df = monthly_page_event_df.withColumnRenamed('avg({})'.format(page), 'avg_monthly_{}'.format(page))\n",
    "\n",
    "    monthly_page_event_df = monthly_page_event_df.drop('Cancel','CancellationConfirmation','month').drop_duplicates()\n",
    "\n",
    "    return daily_page_event_df.join(monthly_page_event_df, on='userId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_all_features(df, df_sess_id, df_sess_duration, df_item, df_page):\n",
    "    \"\"\"\n",
    "    Joins all aggregate dataframes and main dataframe for couple of original columns\n",
    "    :param df: main dataframe\n",
    "    :param df_sess_id: daily,monthly session id features dataframe\n",
    "    :param df_sess_duration: daily,monthly session duration features dataframe\n",
    "    :param df_item: daily,monthly item features dataframe\n",
    "    :param df_page: daily,monthly page events features dataframe\n",
    "    :return features spark dataframe with all generated features\n",
    "    \"\"\"\n",
    "\n",
    "    all_aggs_df =\\\n",
    "    df_sess_id\\\n",
    "    .join(df_sess_duration, on='userId')\\\n",
    "    .join(df_item, on='userId')\\\n",
    "    .join(df_page, on='userId')\n",
    "    \n",
    "    df = df.drop('auth', 'level','length','userAgent','month','date','interaction_time','registration', 'ts','song','page','itemInSession','sessionId','artist','firstName','lastName','method','status')\n",
    "    joined_df = all_aggs_df.join(df, on='userId')\n",
    "    \n",
    "    joined_df = joined_df.drop_duplicates()\n",
    "    features = joined_df.drop('userId')\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(num_cols):\n",
    "    \"\"\"\n",
    "    Adds string indexer for categorical columns\n",
    "    Vector assembler for numerical columns\n",
    "    Creates a pipeline with indexer and assembler\n",
    "    :param num_cols:  list of numerical column names\n",
    "    :return data processing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    indexer_gender = StringIndexer(inputCol='gender', outputCol='gender_index')\n",
    "    indexer_location = StringIndexer(inputCol='location', outputCol='location_index')\n",
    "    indexer_valid_level = StringIndexer(inputCol='valid_level', outputCol='valid_level_index')\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=num_cols, outputCol='features')\n",
    "\n",
    "    process_pipeline = Pipeline(stages=[indexer_gender, indexer_location, indexer_valid_level, assembler])\n",
    "\n",
    "    return process_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_data(features_df):\n",
    "    \"\"\"\n",
    "    Transforms features \n",
    "    \"\"\"\n",
    "    num_cols = []\n",
    "    for field in features_df.schema.fields :\n",
    "        if field.dataType!=StringType():\n",
    "            num_cols.append(field.name)\n",
    "\n",
    "    # num_cols.remove('userId')\n",
    "    num_cols.remove('label')\n",
    "\n",
    "    process_pipeline = build_pipeline(num_cols)\n",
    "    model_df = process_pipeline.fit(features_df).transform(features_df)\n",
    "    return model_df.select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(train, test, model):\n",
    "    \"\"\"\n",
    "    Fits and runs predictions with given training and test data\n",
    "    Displays results for given model\n",
    "    :param train : Spark dataframe with training data\n",
    "    :param test : a Spark data frame with testing data\n",
    "    :param model : model name as string, either 'logistic_regression', 'random_forest' \n",
    "                    or 'gradient_boosting'\n",
    "    :return \n",
    "    \"\"\"\n",
    "\n",
    "    if model == 'logistic_regression':\n",
    "        ml = LogisticRegression()\n",
    "    elif model == 'random_forest':\n",
    "        ml = RandomForestClassifier()\n",
    "    elif model == 'gradient_boosting':\n",
    "        ml = GBTClassifier()\n",
    "    else:\n",
    "        return \"Please choose an appropriate model\"\n",
    "    \n",
    "    # Fit and calculate predictions\n",
    "    classification = ml.fit(train)\n",
    "    results = classification.transform(test)\n",
    "    # Area under ROC\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    print(\"Test set ROC: \" + str(evaluator.evaluate(results, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    \n",
    "    # Calculate accuracy and F-1 score\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "    accuracy = accuracy_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "    \n",
    "    f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    f1_score = f1_score_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "    \n",
    "    print('For {}, the accuracy on the test set is {:.2%} and the F-1 score is {}'\\\n",
    "    .format(model, accuracy, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleanse_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = label_data(df)\n",
    "df = registered_days(df)\n",
    "df = latest_level(df)\n",
    "df = avg_length(df)\n",
    "df_sess_id = sessionid_aggregates(df)\n",
    "df_sess_duration = session_duration_aggregates(df)\n",
    "df_item = item_aggregates(df)\n",
    "df_page = page_event_aggregates(df)\n",
    "\n",
    "features_df = join_all_features(df, df_sess_id, df_sess_duration, df_item, df_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_daily_sessions</th>\n",
       "      <th>avg_monthly_sessions</th>\n",
       "      <th>avg_daily_session_duration</th>\n",
       "      <th>avg_monthly_session_duration</th>\n",
       "      <th>avg_daily_items</th>\n",
       "      <th>avg_monthly_items</th>\n",
       "      <th>avg_daily_RollAdvert</th>\n",
       "      <th>avg_daily_Settings</th>\n",
       "      <th>avg_daily_Downgrade</th>\n",
       "      <th>avg_daily_NextSong</th>\n",
       "      <th>avg_daily_Error</th>\n",
       "      <th>avg_daily_About</th>\n",
       "      <th>avg_daily_Upgrade</th>\n",
       "      <th>avg_daily_Home</th>\n",
       "      <th>avg_daily_Logout</th>\n",
       "      <th>avg_daily_AddtoPlaylist</th>\n",
       "      <th>avg_daily_ThumbsDown</th>\n",
       "      <th>avg_daily_ThumbsUp</th>\n",
       "      <th>avg_daily_SaveSettings</th>\n",
       "      <th>avg_daily_AddFriend</th>\n",
       "      <th>avg_daily_SubmitUpgrade</th>\n",
       "      <th>avg_daily_Help</th>\n",
       "      <th>avg_daily_SubmitDowngrade</th>\n",
       "      <th>avg_monthly_RollAdvert</th>\n",
       "      <th>avg_monthly_Settings</th>\n",
       "      <th>avg_monthly_Downgrade</th>\n",
       "      <th>avg_monthly_NextSong</th>\n",
       "      <th>avg_monthly_Error</th>\n",
       "      <th>avg_monthly_About</th>\n",
       "      <th>avg_monthly_Upgrade</th>\n",
       "      <th>avg_monthly_Home</th>\n",
       "      <th>avg_monthly_Logout</th>\n",
       "      <th>avg_monthly_AddtoPlaylist</th>\n",
       "      <th>avg_monthly_ThumbsDown</th>\n",
       "      <th>avg_monthly_ThumbsUp</th>\n",
       "      <th>avg_monthly_SaveSettings</th>\n",
       "      <th>avg_monthly_AddFriend</th>\n",
       "      <th>avg_monthly_SubmitUpgrade</th>\n",
       "      <th>avg_monthly_Help</th>\n",
       "      <th>avg_monthly_SubmitDowngrade</th>\n",
       "      <th>last_interaction</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "      <th>registered_days</th>\n",
       "      <th>valid_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12856.333333</td>\n",
       "      <td>19435.500000</td>\n",
       "      <td>67.666667</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1540856767000</td>\n",
       "      <td>M</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.285714</td>\n",
       "      <td>19.5</td>\n",
       "      <td>10934.466667</td>\n",
       "      <td>12644.206522</td>\n",
       "      <td>74.742857</td>\n",
       "      <td>211.5</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>54.485714</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.214286</td>\n",
       "      <td>1.722222</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>953.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1543536707000</td>\n",
       "      <td>M</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.083333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4033.750000</td>\n",
       "      <td>5306.375000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>83.5</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1542726950000</td>\n",
       "      <td>M</td>\n",
       "      <td>FL</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.320000</td>\n",
       "      <td>13.5</td>\n",
       "      <td>14999.980000</td>\n",
       "      <td>16690.200549</td>\n",
       "      <td>106.240000</td>\n",
       "      <td>461.0</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>74.250000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.304348</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.538462</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>891.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>44.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1543525111000</td>\n",
       "      <td>M</td>\n",
       "      <td>NH</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7729.333333</td>\n",
       "      <td>6241.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>51.5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1542594291000</td>\n",
       "      <td>M</td>\n",
       "      <td>WA</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_daily_sessions  avg_monthly_sessions  avg_daily_session_duration  avg_monthly_session_duration  avg_daily_items  avg_monthly_items  avg_daily_RollAdvert  avg_daily_Settings  avg_daily_Downgrade  avg_daily_NextSong  avg_daily_Error  avg_daily_About  avg_daily_Upgrade  avg_daily_Home  avg_daily_Logout  avg_daily_AddtoPlaylist  avg_daily_ThumbsDown  avg_daily_ThumbsUp  avg_daily_SaveSettings  avg_daily_AddFriend  avg_daily_SubmitUpgrade  avg_daily_Help  avg_daily_SubmitDowngrade  avg_monthly_RollAdvert  avg_monthly_Settings  avg_monthly_Downgrade  avg_monthly_NextSong  avg_monthly_Error  avg_monthly_About  avg_monthly_Upgrade  avg_monthly_Home  avg_monthly_Logout  avg_monthly_AddtoPlaylist  avg_monthly_ThumbsDown  avg_monthly_ThumbsUp  avg_monthly_SaveSettings  avg_monthly_AddFriend  avg_monthly_SubmitUpgrade  avg_monthly_Help  avg_monthly_SubmitDowngrade  last_interaction gender location  label  registered_days valid_level\n",
       "0  1.000000            2.0                   12856.333333                19435.500000                  67.666667        124.0              0.000000              1.000000            0.000000             43.000000           0.00             1.00             2.000000           3.333333        3.000000          2.000000                 0.000000              3.500000            0.0                     2.666667             1.0                      0.000000        0.0                        0.0                     1.0                   0.0                    129.0                 0.0                1.0                2.0                  10.0              3.0                 4.0                        0.0                     7.0                   0.0                       8.0                    1.0                        0.0               0.0                          1540856767000     M      CA       0      46               paid      \n",
       "1  1.285714            19.5                  10934.466667                12644.206522                  74.742857        211.5              3.842105              1.083333            1.125000             54.485714           1.25             1.25             1.333333           3.214286        1.722222          2.285714                 1.416667              3.461538            1.0                     2.857143             1.0                      1.555556        1.0                        36.5                    6.5                   4.5                    953.5                 5.0                2.5                2.0                  45.0              15.5                24.0                       8.5                     45.0                  1.0                       20.0                   1.0                        7.0               1.0                          1543536707000     M      CA       0      75               paid      \n",
       "2  1.083333            6.0                   4033.750000                 5306.375000                   28.750000        83.5               4.625000              0.000000            0.000000             24.900000           0.00             0.00             2.000000           1.900000        1.125000          3.000000                 1.000000              1.285714            0.0                     1.500000             0.0                      1.500000        0.0                        18.5                    0.0                   0.0                    124.5                 0.0                0.0                2.0                  9.5               4.5                 3.0                        1.0                     4.5                   0.0                       3.0                    0.0                        1.5               0.0                          1542726950000     M      FL       0      181              free      \n",
       "3  1.320000            13.5                  14999.980000                16690.200549                  106.240000       461.0              3.111111              2.000000            1.909091             74.250000           1.00             1.00             1.400000           3.304348        1.500000          3.538462                 1.875000              5.933333            1.0                     2.400000             1.0                      1.600000        1.0                        14.0                    7.0                   10.5                   891.0                 1.0                3.0                3.5                  38.0              12.0                23.0                       7.5                     44.5                  1.5                       18.0                   1.0                        4.0               1.0                          1543525111000     M      NH       0      71               paid      \n",
       "4  1.000000            1.5                   7729.333333                 6241.000000                   40.000000        51.5               3.500000              0.000000            0.000000             33.000000           0.00             0.00             1.000000           1.000000        1.000000          2.000000                 1.000000              2.500000            0.0                     0.000000             0.0                      0.000000        0.0                        7.0                     0.0                   0.0                    49.5                  0.0                0.0                1.0                  1.5               1.0                 2.0                        2.0                     2.5                   0.0                       0.0                    0.0                        0.0               0.0                          1542594291000     M      WA       0      71               free      "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features_df.take(5), columns=features_df.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = post_process_data(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test subsets\n",
    "train, test = model_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC: 0.7680055401662049\n",
      "For logistic_regression, the accuracy on the test set is 77.89% and the F-1 score is 0.7508245877061469\n",
      "Test set ROC: 0.9411357340720221\n",
      "For random_forest, the accuracy on the test set is 86.32% and the F-1 score is 0.832969696969697\n",
      "Test set ROC: 0.8732686980609417\n",
      "For gradient_boosting, the accuracy on the test set is 84.21% and the F-1 score is 0.8462923555410051\n"
     ]
    }
   ],
   "source": [
    "# Fit various models and visualize their accuracies\n",
    "for model in ['logistic_regression', 'random_forest', 'gradient_boosting']:\n",
    "    fit_predict(train, test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 90.53% and the F-1 score is 0.8966524650030432\n",
      "avg_daily_sessions : 0.04227738966184244 \n",
      "\n",
      "avg_monthly_sessions : 0.028785800998496012 \n",
      "\n",
      "avg_daily_session_duration : 0.01916529093511879 \n",
      "\n",
      "avg_monthly_session_duration : 0.0230303487929468 \n",
      "\n",
      "avg_daily_items : 0.017693859907340296 \n",
      "\n",
      "avg_monthly_items : 0.015942747491309934 \n",
      "\n",
      "avg_daily_RollAdvert : 0.018173400477590286 \n",
      "\n",
      "avg_daily_Settings : 0.012228430086885617 \n",
      "\n",
      "avg_daily_Downgrade : 0.019223821083351623 \n",
      "\n",
      "avg_daily_NextSong : 0.019112581982666382 \n",
      "\n",
      "avg_daily_Error : 0.006485700854203084 \n",
      "\n",
      "avg_daily_About : 0.006519009728806542 \n",
      "\n",
      "avg_daily_Upgrade : 0.0082732178785747 \n",
      "\n",
      "avg_daily_Home : 0.024159971693419945 \n",
      "\n",
      "avg_daily_Logout : 0.018632966046365818 \n",
      "\n",
      "avg_daily_AddtoPlaylist : 0.01888572606243551 \n",
      "\n",
      "avg_daily_ThumbsDown : 0.023140799640797887 \n",
      "\n",
      "avg_daily_ThumbsUp : 0.013937734030735005 \n",
      "\n",
      "avg_daily_SaveSettings : 0.0030153891887023804 \n",
      "\n",
      "avg_daily_AddFriend : 0.02311710197069498 \n",
      "\n",
      "avg_daily_SubmitUpgrade : 0.006378088368164687 \n",
      "\n",
      "avg_daily_Help : 0.019077003257940726 \n",
      "\n",
      "avg_daily_SubmitDowngrade : 0.001676479184643706 \n",
      "\n",
      "avg_monthly_RollAdvert : 0.0436661645217013 \n",
      "\n",
      "avg_monthly_Settings : 0.00977727666084523 \n",
      "\n",
      "avg_monthly_Downgrade : 0.017005744150244837 \n",
      "\n",
      "avg_monthly_NextSong : 0.016896367441419667 \n",
      "\n",
      "avg_monthly_Error : 0.013219958653287543 \n",
      "\n",
      "avg_monthly_About : 0.008509566134840446 \n",
      "\n",
      "avg_monthly_Upgrade : 0.02464679164750814 \n",
      "\n",
      "avg_monthly_Home : 0.017833753939227148 \n",
      "\n",
      "avg_monthly_Logout : 0.012245040469834389 \n",
      "\n",
      "avg_monthly_AddtoPlaylist : 0.013047922750101372 \n",
      "\n",
      "avg_monthly_ThumbsDown : 0.01604311224091838 \n",
      "\n",
      "avg_monthly_ThumbsUp : 0.01721088913476061 \n",
      "\n",
      "avg_monthly_SaveSettings : 0.007380974032643411 \n",
      "\n",
      "avg_monthly_AddFriend : 0.020662918170852577 \n",
      "\n",
      "avg_monthly_SubmitUpgrade : 0.013734152219153799 \n",
      "\n",
      "avg_monthly_Help : 0.01597262147755108 \n",
      "\n",
      "avg_monthly_SubmitDowngrade : 0.004110591839013685 \n",
      "\n",
      "last_interaction : 0.2622822860134997 \n",
      "\n",
      "gender : 0.07682100917956354 \n",
      "\n",
      "max depth:10\n",
      "num Trees:75\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "clf = RandomForestClassifier()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(clf.numTrees, [20,75]) \\\n",
    "    .addGrid(clf.maxDepth, [10,20]) \\\n",
    "    .build()\n",
    "crossval = CrossValidator(estimator = Pipeline(stages=[clf]),\n",
    "                         estimatorParamMaps = paramGrid,\n",
    "                         evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                         numFolds = 3)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions.select(col('label'), col('prediction')))\n",
    "\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score = f1_score_evaluator.evaluate(predictions.select(col('label'), col('prediction')))\n",
    "bestPipeline = cvModel.bestModel\n",
    "\n",
    "print('The accuracy on the test set is {:.2%} and the F-1 score is {}'\\\n",
    ".format(accuracy, f1_score))\n",
    "# prints feature importances\n",
    "for i in range(len(bestPipeline.stages[0].featureImportances)):\n",
    "    print(\"{} : {} \\n\".format(features_df.columns[i], bestPipeline.stages[0].featureImportances[i]))\n",
    "\n",
    "print('max depth:{}'.format(bestPipeline.stages[0].getOrDefault('maxDepth')))\n",
    "print('num Trees:{}'.format(bestPipeline.stages[0].getNumTrees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
